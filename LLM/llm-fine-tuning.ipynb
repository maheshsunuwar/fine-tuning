{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert'>\n",
    "\n",
    "### Fine Tuning LLM\n",
    "\n",
    "</div>\n",
    "\n",
    "Transforming a foundation LLM into a solution that meets an applications's nees requires a process called *`fine-tuning`*.\n",
    "\n",
    "A secondaryh process called *`distillation`* generates a small (fewer parameters) version of the fine-tuned model.\n",
    "\n",
    "Despite the relatively tiny number of training examples, standard fine-tuning is often computationally expensive. That's because standard fine-tuning involves updating the weight and bias of every parameter on each backpropagation iteration. Fortunately, a smarter process called `parameter-efficient` tuning can fine-tune an LLM by adjusting only a `subset of parameters` on each backpropagation iteration.\n",
    "\n",
    "A fine-tuned model's predictions are usually better than the foundation LLM's predictions. However, a fine-tuned model contains the same number of parameters as the foundation LLM. So, *if a foundation LLM contains ten billion parameters, then the fine-tuned version will also contain ten billion parameters*.\n",
    "\n",
    "This is where distillation comes in.\n",
    "\n",
    "*`Distillation`* creates a smaller version of an LLM. THe distilled LLM generates predictions much faster and requires fewer computational and environmental resources than the full LLM. However, the distilled model's predictions are generally not quite as good as the original LLM's predictions. \n",
    "\n",
    "*`Remember`*: LLMs with more parameters almost always generate better predictions than LLMs with fewer parameters.\n",
    "\n",
    "Find more concepts here: https://developers.google.com/machine-learning/crash-course/llm/tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-success'>\n",
    "Lets Start.\n",
    "\n",
    "We will follow these steps:\n",
    "1. Choose a pre-trained model and a dataset\n",
    "2. Load the data\n",
    "3. Tokenizer\n",
    "4. Initialize our base model\n",
    "5. Evaluate the method\n",
    "6. Fine tune using the Trainer method\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert'>\n",
    "\n",
    "#### 1. Choose a pre-trained model and a dataset\n",
    "\n",
    "</div>\n",
    "\n",
    "Lets use *`google-bert/bert-base-uncased`* model and a *`twitter dataset`*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert'>\n",
    "\n",
    "#### 2. Load the dataset\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'text', 'label', 'label_text'],\n",
       "        num_rows: 27481\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'text', 'label', 'label_text'],\n",
       "        num_rows: 3534\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "twitter_dataset = load_dataset(\"mteb/tweet_sentiment_extraction\")\n",
    "twitter_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cb774db0d1</th>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549e992a42</th>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>088c60f138</th>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9642c003ef</th>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358bd9e861</th>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         text  label  \\\n",
       "id                                                                     \n",
       "cb774db0d1                I`d have responded, if I were going      1   \n",
       "549e992a42      Sooo SAD I will miss you here in San Diego!!!      0   \n",
       "088c60f138                          my boss is bullying me...      0   \n",
       "9642c003ef                     what interview! leave me alone      0   \n",
       "358bd9e861   Sons of ****, why couldn`t they put them on t...      0   \n",
       "\n",
       "           label_text  \n",
       "id                     \n",
       "cb774db0d1    neutral  \n",
       "549e992a42   negative  \n",
       "088c60f138   negative  \n",
       "9642c003ef   negative  \n",
       "358bd9e861   negative  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pprint\n",
    "\n",
    "training_dataset = pd.DataFrame(twitter_dataset['train'])\n",
    "testing_dataset = pd.DataFrame(twitter_dataset['test'])\n",
    "\n",
    "training_dataset.set_index('id', inplace=True)\n",
    "testing_dataset.set_index('id', inplace=True)\n",
    "\n",
    "training_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27481, 3),\n",
       " (3534, 3),\n",
       " array(['neutral', 'negative', 'positive'], dtype=object))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset.shape, testing_dataset.shape, training_dataset['label_text'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the labels are `neutral`, `negative`, and `positive`.\n",
    "\n",
    "So our goal is to classify the sentiment of the `text` into those `labels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11118, 7781, 8582)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if the labels have balanced data\n",
    "total_neutral = len(training_dataset[training_dataset['label'] == 1])\n",
    "total_negative = len(training_dataset[training_dataset['label'] == 0])\n",
    "total_positive = len(training_dataset[training_dataset['label'] == 2])\n",
    "\n",
    "total_neutral, total_negative, total_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 3 artists>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.barh(['neutral', 'negative', 'positive'], [total_neutral, total_negative, total_positive])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like neutral has more data. but its not a lot, so it is acceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert'>\n",
    "\n",
    "#### 3. Load the tokenizer\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As LLMs work with tokens, we need to tokenize our entire dataset using the tokenizer.\n",
    "\n",
    "We can use map method to apply the preprocessing function over the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "# d = load_dataset(\"mteb/tweet_sentiment_extraction\")\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# t.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(observation):\n",
    "    return tokenizer(observation['text'], padding='max_length', truncation=True)\n",
    "\n",
    "tokenized_dataset = twitter_dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['id', 'text', 'label', 'label_text', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 27481\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['id', 'text', 'label', 'label_text', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 3534\n",
       " }))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['train'], tokenized_dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert'>\n",
    "\n",
    "#### 4. Initialize our base model\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# model = AutoModelForMaskedLM.from_pretrained('google-bert/bert-base-uncased', num_labels=3)\n",
    "\n",
    "from transformers import GPT2ForSequenceClassification\n",
    "\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", num_labels=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert'>\n",
    "\n",
    "#### 5. Evaluate\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "   logits, labels = eval_pred\n",
    "   predictions = np.argmax(logits, axis=-1)\n",
    "   return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert'>\n",
    "\n",
    "#### 6. Fine-tune the model\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20610' max='20610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20610/20610 3:52:48, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.883400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.723400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.725400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.719200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.657300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.681800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.663300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.655700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.669000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.679600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.659100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.675200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.677100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.593100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.594000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.580800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.590500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.567600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.599200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.546600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.622300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.598500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.587800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.589500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.583300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.611200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.559700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.533300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.504000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.503900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.482000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.541400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.442200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.547300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.465900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.479200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.479000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.487100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.429200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.498400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.516900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20610, training_loss=0.5891319002820589, metrics={'train_runtime': 13970.0241, 'train_samples_per_second': 5.901, 'train_steps_per_second': 1.475, 'total_flos': 4.307986164901478e+16, 'train_loss': 0.5891319002820589, 'epoch': 2.9996725010006915})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "   output_dir=\"test_trainer\",\n",
    "   #evaluation_strategy=\"epoch\",\n",
    "   per_device_train_batch_size=1,  # Reduce batch size here\n",
    "   per_device_eval_batch_size=1,    # Optionally, reduce for evaluation as well\n",
    "   gradient_accumulation_steps=4\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=tokenized_dataset['train'],\n",
    "   eval_dataset=tokenized_dataset['test'],\n",
    "   compute_metrics=compute_metrics,\n",
    "\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
